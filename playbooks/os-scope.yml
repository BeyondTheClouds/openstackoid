#   ____                ______           __        _    __
#  / __ \___  ___ ___  / __/ /____ _____/ /_____  (_)__/ /
# / /_/ / _ \/ -_) _ \_\ \/ __/ _ `/ __/  '_/ _ \/ / _  /
# \____/ .__/\__/_//_/___/\__/\_,_/\__/_/\_\\___/_/\_,_/
#     /_/
# Make your OpenStacks Collaborative
#
# Playbook that setups the scope interpretation.
#

---
- hosts: all
  vars:
    os_services: [
      g-api, g-reg, keystone, n-api-meta, n-api, n-cauth,
      n-cond-cell1, n-cpu, n-novnc-cell1, n-sch, n-super-cond,
      placement-api, q-agt, q-dhcp, q-l3, q-meta, q-svc ]
  tasks:

# ------------------------------------------------------------------------
# -- Scaffolding

  - name: Compute the list of services (services.json)
    local_action:
      module: template
      src: haproxy/services.json.j2
      dest: haproxy/services.json
    run_once: True

  - name: Read the values of services.json for later HA configuration
    local_action:
      module: include_vars
      file: haproxy/services.json
    run_once: True

  - name: Install OS-CLI plugin and patched K-middleware to interpret the scope
    pip:
      name: "{{item}}"
      editable: true
    with_items:
      - file:///vagrant/python-openstackoidclient
      - file:///vagrant/keystonemiddleware
    become: true

  # The `endpoint_data_for` method [1] of keystoneauth1 fetches
  # endpoint data from the service catalog. This method may filter
  # endpoints based on the region name. Thus, when a service such a
  # Nova wanna contact Neutron, it uses that method to look into the
  # catalog and only keep endpoints of Neutron in the same region
  # (see, nova.network.neutronV2.api.get_client [2]).
  #
  # However with OpenStackoïd, the Identity server of the current
  # composition may be in a different region than Nova. This is the
  # case when Alice start a VM with the following scope:
  #
  #   openstack server create ... --os-scope '{"identity": "CloudOne", "nova": "CloudTwo"}'
  #
  # In this case, Keystone will return a scoped-token with a catalog
  # made of endpoints in CloudOne. Hence, when Nova try to get the
  # list of endpoints of its region, it fails with the error message:
  #
  #   ['internal', 'public'] endpoint for network service in CloudTwo region not found
  #
  # With OpenStackoïd, the notion of regions doesn't really make sense
  # since we permit collaboration between multiple regions. This
  # monkey patch removes the region filtering when a service seeks for
  # an endpoint in the service catalog.
  #
  # Refs:
  # [1] https://github.com/openstack/keystoneauth/blob/8505f37124bb21f41e346a571057c8133f9ca4d5/keystoneauth1/access/service_catalog.py#L402
  # [2] https://github.com/openstack/nova/blob/3310c3cbf534f5d75477ed206a8fb68eb53c6c10/nova/network/neutronv2/api.py#L203
  - name: Patch keystoneauth1 for not filtering catalog endpoints on region name
    command: sed -i '430s/region_name=region_name/region_name=None/' service_catalog.py
    args:
      chdir: /usr/local/lib/python2.7/dist-packages/keystoneauth1/access/
    become: true
    warn: false

# ------------------------------------------------------------------------
# -- HAProxy with Scope interpretation

  - name: Make a directory for HAProxy
    file:
      path: /etc/haproxy
      state: directory
    become: true

  - name: Render HAProxy configuration file (haproxy.cfg)
    template:
      src: haproxy/haproxy.cfg.j2
      dest: /etc/haproxy/haproxy.cfg
    become: true

  - name: Copy Lua scripts
    copy:
      src: "haproxy/{{ item }}"
      dest: "/etc/haproxy/{{ item }}"
    loop:
      - services.json
      - lua/
    become: true

  - name: Configure $CONFIG and $LUA_PATH for HA service
    # - $CONFIG contains the link of HA conf file
    # - $LUA_PATH contains the link of Lua scripts
    ini_file:
      path: /lib/systemd/system/haproxy.service
      section: Service
      option: Environment
      value: >
        CONFIG=/etc/haproxy/haproxy.cfg
        LUA_PATH=/etc/haproxy/lua/?.lua;
    become: true

  - name: Restart HA
    systemd:
      name: haproxy
      state: restarted
      daemon_reload: yes
    become: true

# ------------------------------------------------------------------------
# -- [HACK] Wriggle out of no HA support in Desvtack

  - name: "[HACK] Update devstack@* services to go through HAProxy"
    ini_file:
      path: "/etc/systemd/system/devstack@{{ item }}.service"
      section: Service
      option: Environment
      value: "HTTP_PROXY=http://{{ current_cloud.ip}}:8888"
    loop: "{{ os_services }}"
    become: true

  - name: "[HACK] Make OS-CLI goes through Keystone Frontend"
    lineinfile:
      path: ~/.bashrc
      line: >
        export OS_AUTH_URL="http://{{ current_cloud.ip }}:8888/identity"
      create: yes
    become: true
    become_user: stack

  - name: "[HACK] Add HAProxy frontend as endpoint"
    os_keystone_endpoint:
      # Look for `devstack-admin` in /etc/openstack/clouds.yaml
      cloud: devstack-admin
      service: "{{ item.service }}"
      endpoint_interface: "{{ item.endpoint }}"
      url: "http://{{ current_cloud.ip }}:{{ item.url }}"
      region: "{{ current_cloud.name }}"
      state: present
    loop:
      - { service: placement,   endpoint: public, url: "8888/placement" }
      - { service: nova_legacy, endpoint: public, url: "8888/compute/v2/$(project_id)s"}
      - { service: nova,        endpoint: public, url: "8888/compute/v2.1"}
      - { service: neutron,     endpoint: public, url: "9797/"}
      - { service: glance,      endpoint: public, url: "8888/image"}
      - { service: keystone,    endpoint: public, url: "8888/identity"}
      - { service: keystone,    endpoint: admin,  url: "8888/identity"}

  - name: Restart devstack@* services
    systemd:
      name: "devstack@{{ item }}"
      state: restarted
      daemon_reload: yes
    loop: "{{ os_services }}"
    become: true
